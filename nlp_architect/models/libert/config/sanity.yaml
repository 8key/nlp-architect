train_batch_size: 16
eval_batch_size: 16
max_seq_length: 64
max_epochs: 1
model_type: bert # bert or li-bert
model_name_or_path: bert-base-uncased
random_init: true

data: sanity # relative to nlp_architect/models/li_bert/data/
splits: [1]
seeds: [12, 57]

li_layer: 5
li_layers: [2]
all_layers: false
relation: ""
parse_probs: true
replace_final: false
duplicated_rels: false
transpose: false

num_workers: 88
gpus: 1

overwrite_cache: false # if false, loads features from cached file (quicker)
cache_dir: # default: nlp-architect/nlp_architect/cache
output_dir: # default: nlp-architect/nlp_architect/cache/li-bert-models
labels: labels.txt # relative to nlp_architect/models/li_bert/data/ 

do_train: true
do_predict: false
adam_epsilon: 1.0e-08
fast_dev_run: false
accumulate_grad_batches: 1
learning_rate: 5.0e-05
gradient_clip_val: 1.0
n_tpu_cores: 0
resume_from_checkpoint: null
tokenizer_name: null
val_check_interval: 1.0
warmup_steps: 0
weight_decay: 0.0
logger: true