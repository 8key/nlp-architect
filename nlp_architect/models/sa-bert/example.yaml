adam_epsilon: 1.0e-08
cache_dir: /home/daniel_nlp/nlp-architect/nlp_architect/cache
config_name: ''
data_dir: /home/daniel_nlp/nlp-architect/nlp_architect/models/sa-bert/example/
do_predict: false
do_train: true
eval_batch_size: 32
fast_dev_run: false
fp16: false
fp16_opt_level: '01'
gpus: 4
gradient_accumulation_steps: 1
labels: /home/daniel_nlp/nlp-architect/nlp_architect/models/sa-bert/example/labels.txt
learning_rate: 5.0e-05
max_grad_norm: 1.0
max_seq_length: 128
model_name_or_path: bert-base-multilingual-cased
n_tpu_cores: 0
num_train_epochs: 1
num_workers: 88
output_dir: /home/daniel_nlp/nlp-architect/nlp_architect/cache
overwrite_cache: false
resume_from_checkpoint: null
seed: 1
tokenizer_name: null
train_batch_size: 32
val_check_interval: 1.0
warmup_steps: 0
weight_decay: 0.0
